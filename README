dist_lda

Lightweight python implementation of a distributed, collapsed gibbs sampler for LDA. Uses redis to coordinate multiple nodes.


dist_lda uses a dirty transaction model where words might be missing from local counts and totals in order to improve performance at the expense of further approximating the markov chain.

TODO:
-- Shard topics over multiple redis servers (redis_cluster?)
-- enumerate strings or otherwise low-bit hash to reduce mem footprint
-- invert topic->word hashes to be word->topic . This way each word string is only stored once in redis, at the cost of significantly more pipelining
-- massive amount of benchmarking


BUGS:
-- If individual processes die and restart, you'll get duplicate zombie words in the global state; fixing this would require significant re-architecting and would probably be too slow
